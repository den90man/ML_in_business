{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1  2  3  4  5    6  7     8  9\n",
       "0 -0.33  0.69  0  1  1  0  0.8  0  0.88  N\n",
       "1 -0.33  0.94  1  0  1  0  0.8  1  0.31  O\n",
       "2 -0.33  0.50  1  0  0  0  1.0 -1  0.50  N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"fertility_Diagnosis.txt\", header=None)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть 4 признака и 1 целевая переменная (бинарная) - нужно определить поддельная купюра или нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-65bbc4c1ab30>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-65bbc4c1ab30>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    2Ы\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2Ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, -1] = data.iloc[:, -1].replace({'N':1, 'O':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1  2  3  4  5    6  7     8  9\n",
       "0 -0.33  0.69  0  1  1  0  0.8  0  0.88  1\n",
       "1 -0.33  0.94  1  0  1  0  0.8  1  0.31  0\n",
       "2 -0.33  0.50  1  0  0  0  1.0 -1  0.50  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    88\n",
       "0    12\n",
       "Name: 9, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем выборку на тренировочную и тестовую части и обучаем модель (в примере - градиентный бустинг)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = data.iloc[:,:-1]\n",
    "y_data = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denva\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:56:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 88.24%\n",
      "roc: 66.67%\n",
      "recall: 83.33%\n",
      "precision: 93.75%\n",
      "PR-curve: 96.04%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predict)\n",
    "def evaluate_results(y_test, y_predict):\n",
    "    print('Classification results:')\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "    print(\"f1: %.2f%%\" % (f1 * 100.0)) \n",
    "    roc = roc_auc_score(y_test, y_predict)\n",
    "    print(\"roc: %.2f%%\" % (roc * 100.0)) \n",
    "    rec = recall_score(y_test, y_predict, average='binary')\n",
    "    print(\"recall: %.2f%%\" % (rec * 100.0)) \n",
    "    prc = precision_score(y_test, y_predict, average='binary')\n",
    "    print(\"precision: %.2f%%\" % (prc * 100.0)) \n",
    "    pr_c = auc(recall, precision)\n",
    "    print(\"PR-curve: %.2f%%\" % (pr_c * 100.0)) \n",
    "\n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PU learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что нам неизвестны негативы и часть позитивов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9/88 as positives and unlabeling the rest\n"
     ]
    }
   ],
   "source": [
    "mod_data = data.copy()\n",
    "#get the indices of the positives samples\n",
    "pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "#shuffle them\n",
    "np.random.shuffle(pos_ind)\n",
    "# leave just 10% of the positives marked\n",
    "pos_sample_len = int(np.ceil(0.1 * len(pos_ind)))\n",
    "print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "pos_sample = pos_ind[:pos_sample_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем столбец для новой целевой переменной, где у нас два класса - P (1) и U (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target variable:\n",
      " -1    91\n",
      " 1     9\n",
      "Name: class_test, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mod_data['class_test'] = -1\n",
    "mod_data.loc[pos_sample,'class_test'] = 1\n",
    "print('target variable:\\n', mod_data.iloc[:,-1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "y_positive = mod_data.iloc[:,-2].values # original class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. random negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 11) (9, 11)\n"
     ]
    }
   ],
   "source": [
    "mod_data = mod_data.sample(frac=1)\n",
    "neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "pos_sample = mod_data[mod_data['class_test']==1]\n",
    "print(neg_sample.shape, pos_sample.shape)\n",
    "sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:57:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 92.11%\n",
      "roc: 50.00%\n",
      "recall: 100.00%\n",
      "precision: 85.37%\n",
      "PR-curve: 96.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denva\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#10% позитивных ответов\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 11) (40, 11)\n"
     ]
    }
   ],
   "source": [
    "mod_data = data.copy()\n",
    "#get the indices of the positives samples\n",
    "pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "#shuffle them\n",
    "np.random.shuffle(pos_ind)\n",
    "# leave just 25% of the positives marked\n",
    "pos_sample_len = int(np.ceil(0.45 * len(pos_ind)))\n",
    "#print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "pos_sample = pos_ind[:pos_sample_len]\n",
    "mod_data['class_test'] = -1\n",
    "mod_data.loc[pos_sample,'class_test'] = 1\n",
    "##print('target variable:\\n', mod_data.iloc[:,-1].value_counts())\n",
    "x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "y_positive = mod_data.iloc[:,-2].values # original class\n",
    "mod_data = mod_data.sample(frac=1)\n",
    "neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "pos_sample = mod_data[mod_data['class_test']==1]\n",
    "print(neg_sample.shape, pos_sample.shape)\n",
    "sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:59:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 94.12%\n",
      "roc: 75.00%\n",
      "recall: 100.00%\n",
      "precision: 88.89%\n",
      "PR-curve: 96.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denva\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#45% позитивных ответов\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:58:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 91.18%\n",
      "roc: 57.14%\n",
      "recall: 100.00%\n",
      "precision: 83.78%\n",
      "PR-curve: 96.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denva\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#35% позитивных ответов\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:57:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Classification results:\n",
      "f1: 92.31%\n",
      "roc: 50.00%\n",
      "recall: 100.00%\n",
      "precision: 85.71%\n",
      "PR-curve: 96.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denva\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#25% позитивных ответов\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проверим lookalike на GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 84.85%\n",
      "roc: 63.89%\n",
      "recall: 77.78%\n",
      "precision: 93.33%\n",
      "PR-curve: 95.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predict)\n",
    "def evaluate_results(y_test, y_predict):\n",
    "    print('Classification results:')\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "    print(\"f1: %.2f%%\" % (f1 * 100.0)) \n",
    "    roc = roc_auc_score(y_test, y_predict)\n",
    "    print(\"roc: %.2f%%\" % (roc * 100.0)) \n",
    "    rec = recall_score(y_test, y_predict, average='binary')\n",
    "    print(\"recall: %.2f%%\" % (rec * 100.0)) \n",
    "    prc = precision_score(y_test, y_predict, average='binary')\n",
    "    print(\"precision: %.2f%%\" % (prc * 100.0)) \n",
    "    pr_c = auc(recall, precision)\n",
    "    print(\"PR-curve: %.2f%%\" % (pr_c * 100.0)) \n",
    "\n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PU learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 11) (40, 11)\n"
     ]
    }
   ],
   "source": [
    "mod_data = data.copy()\n",
    "#get the indices of the positives samples\n",
    "pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "#shuffle them\n",
    "np.random.shuffle(pos_ind)\n",
    "# leave just 25% of the positives marked\n",
    "pos_sample_len = int(np.ceil(0.45 * len(pos_ind)))\n",
    "#print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "pos_sample = pos_ind[:pos_sample_len]\n",
    "mod_data['class_test'] = -1\n",
    "mod_data.loc[pos_sample,'class_test'] = 1\n",
    "##print('target variable:\\n', mod_data.iloc[:,-1].value_counts())\n",
    "x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "y_positive = mod_data.iloc[:,-2].values # original class\n",
    "mod_data = mod_data.sample(frac=1)\n",
    "neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "pos_sample = mod_data[mod_data['class_test']==1]\n",
    "print(neg_sample.shape, pos_sample.shape)\n",
    "sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 91.43%\n",
      "roc: 62.50%\n",
      "recall: 100.00%\n",
      "precision: 84.21%\n",
      "PR-curve: 95.56%\n"
     ]
    }
   ],
   "source": [
    "#45% позитивных ответов\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 90.91%\n",
      "roc: 61.67%\n",
      "recall: 90.00%\n",
      "precision: 91.84%\n",
      "PR-curve: 95.56%\n"
     ]
    }
   ],
   "source": [
    "#25% позитивных ответов\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 92.75%\n",
      "roc: 58.33%\n",
      "recall: 100.00%\n",
      "precision: 86.49%\n",
      "PR-curve: 95.56%\n"
     ]
    }
   ],
   "source": [
    "#35% позитивных ответов\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 85.50%\n",
      "roc: 73.89%\n",
      "recall: 77.78%\n",
      "precision: 94.92%\n",
      "PR-curve: 95.56%\n"
     ]
    }
   ],
   "source": [
    "#10% позитивных ответов\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проверим lookalike на RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 91.43%\n",
      "roc: 69.44%\n",
      "recall: 88.89%\n",
      "precision: 94.12%\n",
      "PR-curve: 96.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_predict)\n",
    "def evaluate_results(y_test, y_predict):\n",
    "    print('Classification results:')\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "    print(\"f1: %.2f%%\" % (f1 * 100.0)) \n",
    "    roc = roc_auc_score(y_test, y_predict)\n",
    "    print(\"roc: %.2f%%\" % (roc * 100.0)) \n",
    "    rec = recall_score(y_test, y_predict, average='binary')\n",
    "    print(\"recall: %.2f%%\" % (rec * 100.0)) \n",
    "    prc = precision_score(y_test, y_predict, average='binary')\n",
    "    print(\"precision: %.2f%%\" % (prc * 100.0)) \n",
    "    pr_c = auc(recall, precision)\n",
    "    print(\"PR-curve: %.2f%%\" % (pr_c * 100.0)) \n",
    "\n",
    "evaluate_results(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь очередь за PU learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 11) (22, 11)\n"
     ]
    }
   ],
   "source": [
    "mod_data = data.copy()\n",
    "#get the indices of the positives samples\n",
    "pos_ind = np.where(mod_data.iloc[:,-1].values == 1)[0]\n",
    "#shuffle them\n",
    "np.random.shuffle(pos_ind)\n",
    "# leave just 25% of the positives marked\n",
    "pos_sample_len = int(np.ceil(0.25 * len(pos_ind)))\n",
    "#print(f'Using {pos_sample_len}/{len(pos_ind)} as positives and unlabeling the rest')\n",
    "pos_sample = pos_ind[:pos_sample_len]\n",
    "mod_data['class_test'] = -1\n",
    "mod_data.loc[pos_sample,'class_test'] = 1\n",
    "##print('target variable:\\n', mod_data.iloc[:,-1].value_counts())\n",
    "x_data = mod_data.iloc[:,:-2].values # just the X \n",
    "y_labeled = mod_data.iloc[:,-1].values # new class (just the P & U)\n",
    "y_positive = mod_data.iloc[:,-2].values # original class\n",
    "mod_data = mod_data.sample(frac=1)\n",
    "neg_sample = mod_data[mod_data['class_test']==-1][:len(mod_data[mod_data['class_test']==1])]\n",
    "sample_test = mod_data[mod_data['class_test']==-1][len(mod_data[mod_data['class_test']==1]):]\n",
    "pos_sample = mod_data[mod_data['class_test']==1]\n",
    "print(neg_sample.shape, pos_sample.shape)\n",
    "sample_train = pd.concat([neg_sample, pos_sample]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 90.00%\n",
      "roc: 66.67%\n",
      "recall: 100.00%\n",
      "precision: 81.82%\n",
      "PR-curve: 96.50%\n"
     ]
    }
   ],
   "source": [
    "#50% позитивных ответов\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 93.20%\n",
      "roc: 56.25%\n",
      "recall: 100.00%\n",
      "precision: 87.27%\n",
      "PR-curve: 96.50%\n"
     ]
    }
   ],
   "source": [
    "#25% позитивных ответов\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 92.81%\n",
      "roc: 50.00%\n",
      "recall: 100.00%\n",
      "precision: 86.59%\n",
      "PR-curve: 96.50%\n"
     ]
    }
   ],
   "source": [
    "#10% позитивных ответов\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results:\n",
      "f1: 91.43%\n",
      "roc: 50.00%\n",
      "recall: 100.00%\n",
      "precision: 84.21%\n",
      "PR-curve: 96.50%\n"
     ]
    }
   ],
   "source": [
    "#35% позитивных ответов\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(sample_train.iloc[:,:-2].values, \n",
    "          sample_train.iloc[:,-2].values)\n",
    "y_predict = model.predict(sample_test.iloc[:,:-2].values)\n",
    "evaluate_results(sample_test.iloc[:,-2].values, y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "\n",
    "1. Доля позитивных примеров с увеличенем начинает улучшать метрики.\n",
    "На мой взгляд в данной модели не так очевидно изменение модели при увеличении параметра p, т.к выборка данных взятых для \n",
    "исследования. Также данные не сбалансированные, что показывает roc метрика.\n",
    "Также исследованно влияние различных классификаторов на применение lookalike:\n",
    "В целом различия в метриках при различных классификаторах не большие, но конкретно в этом примере лучшую метрику показал \n",
    "градиентный бустинг при p= 35%\n",
    "Стоит отметить что из -за маленькой выборки данных p больше 0.5 не рассчитывает модель.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
